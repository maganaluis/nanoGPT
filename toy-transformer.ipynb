{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9615ded7-f33f-4ca2-9721-393933b28b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa3c622-ef8b-433a-8075-d9a64a1471c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # Position indices (0 to max_len - 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the positional encodings once in log space\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)  # Not a parameter, but should persist with the model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)  # Add positional encoding\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5587fddc-8d1e-4ff6-8d21-e0c505c9eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads  # Dimension of each head\n",
    "        \n",
    "        # Define linear layers for Q, K, V\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final linear layer after concatenation\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_Q(Q)  # Shape: (batch_size, seq_len_q, d_model)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)  # Shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.depth)  # Shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)  # Shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)  # Shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # Shape: (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.linear(output)  # Shape: (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd425ccb-e69c-4e99-936e-ab799bd00aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)  # Shape: (batch_size, seq_len, d_ff)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e7e537-1efe-4e2e-af23-04defdc98cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-Head Attention with Residual Connection\n",
    "        attn_output, _ = self.multi_head_attention(x, x, x, mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-Forward Network with Residual Connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout2(ff_output)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f0adf1-8a3c-49c3-9c06-869dfbc2034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_len=5000, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5681e9e-0824-4503-9773-07652309d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_len=5000, dropout=0.1):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        encoder_output = self.encoder(src, mask)\n",
    "        logits = self.output_layer(encoder_output)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342c5e2c-b729-4e0d-95b9-bc0adeb21a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 69\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Define the character set (e.g., lowercase letters, digits, punctuation)\n",
    "characters = string.ascii_lowercase + string.digits + string.punctuation + ' '\n",
    "\n",
    "# Create mappings from characters to indices and vice versa\n",
    "char2idx = {char: idx for idx, char in enumerate(characters)}\n",
    "idx2char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "# Update the vocabulary size\n",
    "vocab_size = len(characters)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8e6e5c-3621-4cdb-973f-b1e67c089c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the character-level model\n",
    "d_model = 256       # Embedding dimension\n",
    "num_layers = 8      # Number of transformer blocks\n",
    "num_heads = 4       # Number of attention heads\n",
    "d_ff = 256          # Feed-forward network dimension\n",
    "max_len = 500       # Maximum sequence length (longer sequences for character-level)\n",
    "dropout = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "740a86e6-b593-4c3a-88aa-088d205ec14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"jokes.txt\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1cd1785-f9a9-4ef8-ab7a-5892590fa24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What did one pirate say to the other when he beat him at chess?<>Checkmatey.\\nI burned 2000 calories '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2451ccd8-3ca4-453b-9843-0073ce61b925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Indices: [7, 0, 19, 68, 3, 8, 3, 68, 14, 13, 4, 68, 15, 8, 17, 0, 19, 4, 68, 18, 0, 24, 68, 19, 14, 68, 19, 7, 4, 68, 14, 19, 7, 4, 17, 68, 22, 7, 4, 13, 68, 7, 4, 68, 1, 4, 0, 19, 68, 7, 8, 12, 68, 0, 19, 68, 2, 7, 4, 18, 18, 56, 53, 55, 7, 4, 2, 10, 12, 0, 19, 4, 24, 49, 68, 1, 20, 17, 13, 4, 3, 68, 28, 26, 26, 26, 68, 2, 0, 11, 14, 17, 8, 4, 18, 68, 19, 14, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert text data to indices\n",
    "data_indices = [char2idx[char] for char in text_data if char in char2idx]\n",
    "\n",
    "print(\"Data Indices:\", data_indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb7a1a1b-13d2-472f-b2ab-397e5c616022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 16677\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 256  # Adjust based on your data and memory constraints\n",
    "\n",
    "# Create input and target sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        input_seq = data[i:i+seq_length]\n",
    "        target_seq = data[i+1:i+seq_length+1]\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_seq)\n",
    "    return inputs, targets\n",
    "\n",
    "inputs, targets = create_sequences(data_indices, sequence_length)\n",
    "\n",
    "print(f\"Number of sequences: {len(inputs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad8a7ab-e80e-4d8a-a8cd-de4d440ca53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16677, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs), len(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7447eed-8a60-4df2-8b57-aa6bacdc4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = torch.tensor(self.inputs[idx], dtype=torch.long)\n",
    "        target_seq = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return input_seq, target_seq\n",
    "\n",
    "dataset = CharDataset(inputs, targets)\n",
    "batch_size = 16\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3853fbc-ff8a-4847-8200-53259b1cdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = TransformerLanguageModel(vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6914958d-6be6-42fc-aa84-7ccbc811960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aa15f60-7183-4b39-8942-c0255a7d620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.2032\n",
      "Epoch 2/10, Loss: 1.9236\n",
      "Epoch 3/10, Loss: 1.8821\n",
      "Epoch 4/10, Loss: 1.8648\n",
      "Epoch 5/10, Loss: 1.8566\n",
      "Epoch 6/10, Loss: 1.8500\n",
      "Epoch 7/10, Loss: 1.8455\n",
      "Epoch 8/10, Loss: 1.8406\n",
      "Epoch 9/10, Loss: 1.8385\n",
      "Epoch 10/10, Loss: 1.8355\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        # Move data to device\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(batch_inputs)  # Shape: (batch_size, seq_length, vocab_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits.view(-1, vocab_size), batch_targets.view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d0b8256-24ba-47ab-8fd2-35c7bc61d3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Ok.......................................................................................................\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def generate_text(model, start_string, generation_length=100):\n",
    "    model.eval()\n",
    "    input_indices = [char2idx[char] for char in start_string if char in char2idx]\n",
    "    input_seq = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)  # Shape: (1, seq_length)\n",
    "    \n",
    "    generated_text = start_string\n",
    "    \n",
    "    for _ in range(generation_length):\n",
    "        # Get logits from the model\n",
    "        logits = model(input_seq)  # Shape: (1, seq_length, vocab_size)\n",
    "        next_token_logits = logits[:, -1, :]  # Get logits for the last character\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "        # Sample from the distribution or take the argmax\n",
    "        next_token = torch.argmax(probabilities, dim=-1).item()\n",
    "        # Append the predicted character to the generated text\n",
    "        next_char = idx2char[next_token]\n",
    "        generated_text += next_char\n",
    "        # Update the input sequence\n",
    "        next_token_tensor = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "        input_seq = torch.cat([input_seq, next_token_tensor], dim=1)\n",
    "        # Optionally, keep only the last `sequence_length` tokens\n",
    "        input_seq = input_seq[:, -sequence_length:]\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "start_string = \"Ok...\"\n",
    "generated_text = generate_text(model, start_string, generation_length=100)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad2bd5-be30-47fd-88e4-0c699bbfb70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32859a3-edb4-40ca-8ff6-082f6a8db4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
